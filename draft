#!/bin/bash
echo "Importing CSV into MySQL..."

mysql -u root -p"$MYSQL_ROOT_PASSWORD" -D $MYSQL_DATABASE -e "
CREATE TABLE IF NOT EXISTS orders (
    order_id VARCHAR(100) PRIMARY KEY,
    customer_id VARCHAR(100),
    order_status VARCHAR(20),
    order_purchase_timestamp TIMESTAMP,
    order_approved_at DATETIME,
    order_delivered_carrier_date DATETIME,
    order_delivered_customer_date DATETIME,
    order_estimated_delivery_date DATETIME
);
LOAD DATA INFILE '/var/lib/mysql-files/olist_orders_dataset.csv'
INTO TABLE orders
FIELDS TERMINATED BY ','
ENCLOSED BY '\"'
LINES TERMINATED BY '\n'
IGNORE 1 ROWS
(
    order_id,
    customer_id,
    order_status,
    @order_purchase_timestamp,
    @order_approved_at,
    @order_delivered_carrier_date,
    @order_delivered_customer_date,
    @order_estimated_delivery_date
)
SET order_purchase_timestamp = NULLIF(@order_purchase_timestamp, '')
    , order_approved_at = NULLIF(@order_approved_at, '')
    , order_delivered_carrier_date = NULLIF(@order_delivered_carrier_date, '')
    , order_delivered_customer_date = NULLIF(@order_delivered_customer_date, '')
    , order_estimated_delivery_date = NULLIF(@order_estimated_delivery_date, '')
;
"

echo "CSV import completed."



  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      - broker
    ports:
      - 8080:8080
    env_file:
      - env
    networks:
      - my-network

docker exec -it mysql mysql -u root -p
docker exec -it mysql mysql -u lminhnguyet -p

curl -i -X POST -H "Accept:application/json" -H "Content-Type:application/json" localhost:8083/connectors/ -d '{\
    "name": "mysql-connector",\
    "config": {\
      "connector.class": "io.debezium.connector.mysql.MySqlConnector",\
      "tasks.max": "1",\
      "database.hostname": "mysql",\
      "database.port": "3306",\
      "database.user": "lminhnguyet",\
      "database.password": "123",\
      "database.server.id": "184054",\
      "database.server.name": "dbserver1",\
      "database.include.list": "brazilian_ecom",\
      "database.history.internal.kafka.bootstrap.servers": "kafka:9092",\
      "database.history.internal.kafka.topic": "dbhistory.brazilian_ecom"\
    }\
  }'

  curl -i -X POST -H "Accept:application/json" -H "Content-Type:application/json" localhost:8083/connectors/ -d '{ "name": "mysql-connector01", "config": { "connector.class": "io.debezium.connector.mysql.MySqlConnector", "tasks.max": "1", "database.hostname": "mysql", "database.port": "3306", "database.user": "root", "database.password": "root", "database.server.id": "184054", "topic.prefix": "dbserver1", "database.include.list": "brazilian_ecom", "schema.history.internal.kafka.bootstrap.servers": "kafka:9092", "schema.history.internal.kafka.topic": "schemahistory.brazilian_ecom" } }'
  curl -i -X POST -H "Accept:application/json" -H "Content-Type:application/json" localhost:8083/connectors/ -d '{ "name": "mysql-connector01", "config": { "connector.class": "io.debezium.connector.mysql.MySqlConnector", "tasks.max": "1", "database.hostname": "mysql", "database.port": "3306", "database.user": "root", "database.password": "root", "database.server.id": "184054", "topic.prefix": "dbserver1", "database.include.list": "brazilian_ecom", "schema.history.kafka.bootstrap.servers": "kafka:9092", "schema.history.kafka.topic": "schemahistory.brazilian_ecom" } }'

quyen load data, reload

curl -H "Accept:application/json" localhost:8083/connectors/

1. set up docker-compose.yml < env
2. docker-compose up -d
3. localhost:9089 < kafka ui
  & localhost:9001 < minio ui
4. mysql-connector.json
5. curl -H "Accept:application/json" localhost:8083/connectors/ 
  & curl -i -X POST -H "Accept:application/json" -H "Content-Type:application/json" localhost:8083/connectors/ -d @debezium-mysql-connector.json
  & curl -X DELETE http://localhost:8083/connectors/<connect-name>
6. docker exec -it mysql mysql -u lminhnguyet -p -D brazilian_ecom
7. docker exec -u root -it spark-master bash && spark-submit /app/stream.py

docker exec -it spark-master bash -c "apt-get update && apt-get install -y iputils-ping && ping kafka"
https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.0/spark-token-provider-kafka-0-10_2.12-3.5.0.jar
version: kafka - spark
spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 /app/stream.py
-- old
# version: '3.1'

services:
  mysql:
    image: mysql:8.0
    container_name: mysql
    restart: always
    env_file:
      - env
    ports:
      - 3306:3306
    volumes:
      - mysql_data:/var/lib/mysql
      - ./dataset:/var/lib/mysql-files
      - ./scripts:/docker-entrypoint-initdb.d
    command: --local_infile=1
    networks:
      - my-network
  
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - 2181:2181
    env_file:
      - env
    networks:
      - my-network

  broker:
    image: confluentinc/cp-server:7.4.0
    hostname: broker
    container_name: broker
    depends_on:
      - zookeeper
    ports:
      - 9092:9092
      - 9101:9101
    env_file:
      - env
    networks:
      - my-network

  control-center:
    image: confluentinc/cp-enterprise-control-center:7.4.0
    hostname: control-center
    container_name: control-center
    depends_on:
      - broker
    env_file:
      - env
    ports:
      - 9021:9021
    networks:
      - my-network

  debezium:
    image: debezium/connect:1.9
    container_name: debezium
    depends_on:
      - broker
    ports:
      - 8083:8083
    env_file:
      - env
    volumes:
      - ./kafka-connect:/kafka-connect
    networks:
      - my-network

volumes:
  mysql_data:

networks:
  my-network:


-=----------------------
FROM docker.io/bitnami/spark:3.3

USER root

# Install prerequisites
RUN apt-get update && apt-get install -y curl

RUN curl -O https://repo1.maven.org/maven2/software/amazon/awssdk/s3/2.18.41/s3-2.18.41.jar \
    && curl -O https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.12.367/aws-java-sdk-1.12.367.jar \
    && curl -O https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.2.0/delta-core_2.12-2.2.0.jar \
    && curl -O https://repo1.maven.org/maven2/io/delta/delta-storage/2.2.0/delta-storage-2.2.0.jar \
    && curl -O https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.19/mysql-connector-java-8.0.19.jar \
    && curl -O https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.13/3.3.0/spark-sql-kafka-0-10_2.13-3.3.0.jar \
    && mv s3-2.18.41.jar /opt/bitnami/spark/jars \
    && mv aws-java-sdk-1.12.367.jar /opt/bitnami/spark/jars \
    && mv delta-core_2.12-2.2.0.jar /opt/bitnami/spark/jars \
    && mv delta-storage-2.2.0.jar /opt/bitnami/spark/jars \
    && mv mysql-connector-java-8.0.19.jar /opt/bitnami/spark/jars \
    && mv spark-sql-kafka-0-10_2.13-3.3.0.jar /opt/bitnami/spark/jars


    && curl -O https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.0/spark-sql-kafka-0-10_2.12-3.5.0.jar \
    \
    && mv spark-sql-kafka-0-10_2.12-3.5.0.jar /opt/bitnami/spark/jars